{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Lab V3+ Manual Implementation \n",
    "### With ResNet-50 Backbone for high and low level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n***COMP9517 Group Project: Manual DeepLabV3+/ResNet-50 based Image Segmentation***\\nCreated by: Maximilian Keller (maximilian.keller@unsw.edu.au)\\nDate: 18/07/24\\nPurpose: This script is designed for semantic segmentation using a manually defined DeepLabv3+ model on the WildScenes 2D dataset using a predefined train-val-test split.\\nNotes: Uses entire dataset (uneven class distrinution) through stratified data split by class label.\\n\\nRevision History:\\n\\n18/07/24 - Document created\\n23/07/24 - Implemented baseline DeepLabV3+ model (https://arxiv.org/pdf/1802.02611); (https://medium.com/@r1j1nghimire/semantic-segmentation-using-deeplabv3-from-scratch-b1ff57a27be)\\n27/07/24 - Created Data Loader to handle pre-defined train-val-test file structure (https://github.com/csiro-robotics/WildScenes/tree/main)\\n29/07/24 - First time training on entire Dataset. 4 Epochs of entire dataset. mIoU: 15\\n31/07/24 - Fixed class indexation issue and re-trained model. Drastic improvement in classification accuracy. Only trained 1 Epoch. mIoU: 17\\n31/07/24 - Experimented with different backbones of the ResNet-50 and ResNet-100 models (using different layers). 1 Epoch for each config. mIoU: 17\\n01/08/24 - Finalised script for submission. Added comments and headings.\\n\\nAssumed File Hierarchy:\\n\\nThe script expects a specific file structure for input data, model checkpoints, and output predictions. Change the PATH variables if desired.\\n\\nWildScenes_base_dir/\\n|-- data/\\n|   |-- WildScenes/\\n|       |-- WildScenes2d/\\n|           |-- V-01/\\n|           |   |-- image/\\n|           |   |-- indexLabel/\\n|           |   |-- label/\\n|           |-- V-02/\\n|           |   |-- image/\\n|           |   |-- indexLabel/\\n|           |   |-- label/\\n|           |-- | ....\\n|           |-- Test/\\n|               |-- predictions/                # Folder for RGB predictions output\\n|               |-- predictions_label/          # Folder for indexLabel predictions output\\n|-- splits/\\n|   |-- train.csv                               # CSV file containing training image paths and label paths\\n|   |-- val.csv                                 # CSV file containing validation image paths and label paths\\n|   |-- test.csv                                # CSV file containing test image paths\\n\\nCSV File Format:\\n\\nThe CSV files (train.csv, val.csv, test.csv) should have the following format:\\nid,im_path,label_path\\n1623370893-376340233,WildScenes2d/V-02/image/1623370893-376340233.png,WildScenes2d/V-02/indexLabel/1623370893-376340233.png\\n1623370896-529507864,WildScenes2d/V-02/image/1623370896-529507864.png,WildScenes2d/V-02/indexLabel/1623370896-529507864.png\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "***COMP9517 Group Project: Manual DeepLabV3+/ResNet-50 based Image Segmentation***\n",
    "Author: Maximilian Keller (maximilian.keller@unsw.edu.au)\n",
    "Date: 18/07/24\n",
    "Purpose: This script is designed for semantic segmentation using a manually defined DeepLabv3+ model on the WildScenes 2D dataset using a predefined train-val-test split.\n",
    "Notes: Uses entire dataset (uneven class distrinution) through stratified data split by class label.\n",
    "\n",
    "Revision History:\n",
    "\n",
    "18/07/24 - Document created\n",
    "20/07/24 - Implemented baseline DeepLabV3+ model (https://arxiv.org/pdf/1802.02611); (https://medium.com/@r1j1nghimire/semantic-segmentation-using-deeplabv3-from-scratch-b1ff57a27be)\n",
    "27/07/24 - Created Data Loader to handle pre-defined train-val-test file structure (https://github.com/csiro-robotics/WildScenes/tree/main)\n",
    "29/07/24 - First time training on entire Dataset. 4 Epochs of entire dataset. mIoU: 15\n",
    "31/07/24 - Fixed class indexation issue and re-trained model. Drastic improvement in classification accuracy. Only trained 1 Epoch. mIoU: 17\n",
    "31/07/24 - Experimented with different backbones of the ResNet-50 and ResNet-100 models (using different layers). 4 Epoch for each config. mIoU: 18\n",
    "01/08/24 - Finalised script for submission. Added comments and headings.\n",
    "\n",
    "Assumed File Hierarchy:\n",
    "\n",
    "The script expects a specific file structure for input data, model checkpoints, and output predictions. Change the PATH variables if desired.\n",
    "\n",
    "WildScenes_base_dir/\n",
    "|-- DeepLabV3Plus_Backbone_Study.ipynb          # This Jupyter Notebook\n",
    "|-- data/\n",
    "|   |-- WildScenes/                             # The WildScenes dataset\n",
    "|       |-- WildScenes2d/\n",
    "|           |-- V-01/\n",
    "|           |   |-- image/\n",
    "|           |   |-- indexLabel/\n",
    "|           |   |-- label/\n",
    "|           |-- V-02/\n",
    "|           |   |-- image/\n",
    "|           |   |-- indexLabel/\n",
    "|           |   |-- label/\n",
    "|           |-- | ....\n",
    "|           |-- Test/\n",
    "|               |-- predictions/                # Folder for 'RGB label' predictions output\n",
    "|               |-- predictions_label/          # Folder for 'indexLabel' predictions output\n",
    "|-- splits/\n",
    "|   |-- train.csv                               # CSV file containing training image paths and label paths\n",
    "|   |-- val.csv                                 # CSV file containing validation image paths and label paths\n",
    "|   |-- test.csv                                # CSV file containing test image paths and label paths\n",
    "|-- jupyter_images/                             # Folder containing the images embedded in this notebook\n",
    "\n",
    "CSV File Format:\n",
    "\n",
    "The CSV files (train.csv, val.csv, test.csv) should have the following format:\n",
    "id,im_path,label_path\n",
    "1623370893-376340233,WildScenes2d/V-02/image/1623370893-376340233.png,WildScenes2d/V-02/indexLabel/1623370893-376340233.png\n",
    "1623370896-529507864,WildScenes2d/V-02/image/1623370896-529507864.png,WildScenes2d/V-02/indexLabel/1623370896-529507864.png\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE SELECTION\n",
    "DEVICE = (\n",
    "    \"cuda\"                                                                  # Prefers CUDA acceleration (NVIDIA) -> then Metal acceleration (MAC) -> then CPU\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 4\n",
    "NUM_WORKERS = 0                                                             # Sub-processes used by Data Loader (set to 0 for main thread only)\n",
    "PIN_MEMORY = True\n",
    "NUM_CLASSES = 18\n",
    "IMG_RESIZE_DIM = 800                                                        # Image dimension (in pixels) for training and inference (512 in paper)\n",
    "OUTPUT_DIM = (2016, 1512)                                                   # Output prediction dimension (in pixels) for 'IndexLabel' and 'label'\n",
    "\n",
    "# PATHS\n",
    "TRAIN_CSV = \"splits/train.csv\"                                              # to CSV files (for train-val-test-split to be used in 'CustomDataset' class)\n",
    "VAL_CSV = \"splits/val.csv\"\n",
    "TEST_CSV = \"splits/test.csv\"\n",
    "WILDSCENES_PATH = \"data/WildScenes\"                                         # path to folder containing 'WildScenes2d'\n",
    "MODEL_PATH = \"checkpoint.pth.tar\"                                           # location to write/save training weights + optimiser state / read for testing\n",
    "PREDICTIONS = 'data/WildScenes/WildScenes2d/Test/predictions'               # location to save RGB predictions\n",
    "PREDICTIONS_LABELS = 'data/WildScenes/WildScenes2d/Test/predictions_label'  # location to save 'indexLabel' style index predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for Atrous Convolution filter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atrous_Convolution(nn.Module):\n",
    "    def __init__(self, input_channels, kernel_size, pad, dilation_rate, output_channels=256):\n",
    "        super(Atrous_Convolution, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=kernel_size, padding=pad, dilation=dilation_rate, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Atrous.png](jupyter_images/Atrous-convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for Atrous Spatial Pyramid Pooling (ASSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASSP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASSP, self).__init__()\n",
    "        self.conv_1x1 = Atrous_Convolution(in_channels, 1, 0, 1, out_channels)\n",
    "        self.conv_3x3_6 = Atrous_Convolution(in_channels, 3, 6, 6, out_channels)\n",
    "        self.conv_3x3_12 = Atrous_Convolution(in_channels, 3, 12, 12, out_channels)\n",
    "        self.conv_3x3_18 = Atrous_Convolution(in_channels, 3, 18, 18, out_channels)\n",
    "        self.image_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * 5, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_1x1(x)\n",
    "        x2 = self.conv_3x3_6(x)\n",
    "        x3 = self.conv_3x3_12(x)\n",
    "        x4 = self.conv_3x3_18(x)\n",
    "        x5 = self.image_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x1, x2, x3, x4, x5], dim=1)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ASSP.png](jupyter_images/ASPP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone Feature Extraction Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Torchvision classification models: ['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'googlenet', 'inception_v3', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "print(\"Available Torchvision classification models:\",models.list_models(module=torchvision.models))\n",
    "# print(\"ResNet-50:\",models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2))\n",
    "# print(\"ResNet-101:\",models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2))\n",
    "\n",
    "# Chosen experimental configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"backbone\": \"resnet50\",\n",
    "        \"output_layer_high\": \"layer4\",\n",
    "        \"output_layer_low\": \"layer1\"\n",
    "    },\n",
    "    {\n",
    "        \"backbone\": \"resnet101\",\n",
    "        \"output_layer_high\": \"layer4\",\n",
    "        \"output_layer_low\": \"layer1\"\n",
    "    },\n",
    "    {\n",
    "        \"backbone\": \"resnet50\",\n",
    "        \"output_layer_high\": \"layer3\",\n",
    "        \"output_layer_low\": \"layer1\"\n",
    "    },\n",
    "    {\n",
    "        \"backbone\": \"resnet101\",\n",
    "        \"output_layer_high\": \"layer3\",\n",
    "        \"output_layer_low\": \"layer1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to get the backbone network\n",
    "def get_backbone(name, output_layer):\n",
    "    if name == \"resnet50\":\n",
    "        pretrained_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)       # IMAGENET1K_V1 (accuracy 76.130%); IMAGENET1K_V2 (accuracy 80.858%)\n",
    "    elif name == \"resnet101\":\n",
    "        pretrained_model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)     # IMAGENET1K_V1 (accuracy 77.374%); IMAGENET1K_V2 (accuracy 81.886%)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {name}\")\n",
    "    \n",
    "    layers = list(pretrained_model.children())\n",
    "    if output_layer == 'layer1':  # Up to the first residual block\n",
    "        net = nn.Sequential(*layers[:5])\n",
    "    elif output_layer == 'layer2':  # Up to the second residual block\n",
    "        net = nn.Sequential(*layers[:6])\n",
    "    elif output_layer == 'layer3':  # Up to the third residual block\n",
    "        net = nn.Sequential(*layers[:7])\n",
    "    elif output_layer == 'layer4':  # Up to the fourth residual block\n",
    "        net = nn.Sequential(*layers[:8])\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid output_layer: {output_layer}\")\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepLab V3+ Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deeplabv3Plus(nn.Module):\n",
    "    def __init__(self, num_classes, backbone_name, output_layer_high, output_layer_low):\n",
    "        super(Deeplabv3Plus, self).__init__()\n",
    "        self.backbone = get_backbone(backbone_name, output_layer_high)\n",
    "        self.low_level_features = get_backbone(backbone_name, output_layer_low)\n",
    "        \n",
    "        # Determine the output channels of the high-level features based on the output layer\n",
    "        if output_layer_high == 'layer4':\n",
    "            high_level_channels = 2048\n",
    "        elif output_layer_high == 'layer3':\n",
    "            high_level_channels = 1024\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output layer for high-level features: {output_layer_high}\")\n",
    "        \n",
    "        self.assp = ASSP(in_channels=high_level_channels, out_channels=256)\n",
    "        self.conv1x1 = Atrous_Convolution(256, 1, 0, 1, 48)\n",
    "        self.conv_3x3 = nn.Sequential(\n",
    "            nn.Conv2d(304, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_backbone = self.backbone(x)\n",
    "        x_low_level = self.low_level_features(x)\n",
    "        x_assp = self.assp(x_backbone)\n",
    "\n",
    "        x_assp_upsampled = F.interpolate(x_assp, size=x_low_level.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x_conv1x1 = self.conv1x1(x_low_level)\n",
    "        x_cat = torch.cat([x_conv1x1, x_assp_upsampled], dim=1)\n",
    "        x_3x3 = self.conv_3x3(x_cat)\n",
    "        x_3x3_upscaled = F.interpolate(x_3x3, scale_factor=(4, 4), mode='bilinear', align_corners=True)\n",
    "        x_out = self.classifier(x_3x3_upscaled)\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DeepLabV3plus.png](jupyter_images/Modified_DeepLabV3Plus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from dataset class and define the essential __len__ and __getitem__ methods\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, WildScenes2d_path, transform=None):\n",
    "        self.base_path = WildScenes2d_path\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        label_path = self.data.iloc[idx, 2]\n",
    "        \n",
    "        #print(f\"{self.base_path}/{img_path}\")\n",
    "        image = Image.open(f\"{self.base_path}/{img_path}\").convert(\"RGB\")\n",
    "        label = Image.open(f\"{self.base_path}/{label_path}\").convert(\"RGB\")\n",
    "\n",
    "        image = np.array(image)\n",
    "        label = np.array(label)[:, :, 0]  # Extract the red channel as the class index\n",
    "        label = label - 1  # Shift labels from range 1-18 to 0-17\n",
    "        \n",
    "        # Apply image transform (normalisation)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=label)\n",
    "            image = augmented['image']\n",
    "            label = augmented['mask']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_RESIZE_DIM, IMG_RESIZE_DIM),  # Resize to avoid memory issues\n",
    "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_RESIZE_DIM, IMG_RESIZE_DIM),  # Resize to avoid memory issues\n",
    "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(IMG_RESIZE_DIM, IMG_RESIZE_DIM),  # Resize to avoid memory issues\n",
    "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def get_data_loaders(train_csv, val_csv, test_csv, root_path, batch_size, train_transform, val_transform, test_transform, num_workers=4, pin_memory=True):\n",
    "\n",
    "    train_ds = CustomDataset(csv_file=train_csv, WildScenes2d_path=root_path, transform=train_transform)\n",
    "    val_ds = CustomDataset(csv_file=val_csv, WildScenes2d_path=root_path, transform=val_transform)\n",
    "    test_ds = CustomDataset(csv_file=test_csv, WildScenes2d_path=root_path, transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    WILDSCENES_PATH,\n",
    "    BATCH_SIZE,\n",
    "    train_transform, \n",
    "    val_transform,\n",
    "    test_transform,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 6056\n",
      "Number of validation images: 288\n",
      "Number of testing images: 2136\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_loader)*BATCH_SIZE}\")\n",
    "print(f\"Number of validation images: {len(val_loader)*BATCH_SIZE}\")\n",
    "print(f\"Number of testing images: {len(test_loader)*BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of the training file (train.csv) to use for each training iteration\n",
    "TRAIN_FRACTION = 1\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(config, train_loader, val_loader, num_classes, device, epochs=10, learning_rate=1e-4):\n",
    "    # Instantiate a new DeepLabV3+ model with backbone config\n",
    "    model = Deeplabv3Plus(num_classes=num_classes, \n",
    "                          backbone_name=config[\"backbone\"], \n",
    "                          output_layer_high=config[\"output_layer_high\"], \n",
    "                          output_layer_low=config[\"output_layer_low\"]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_iou = 0\n",
    "    best_metrics = {}\n",
    "    # Main training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # Determine the number of batches to process based on train_fraction\n",
    "        total_batches = int(len(train_loader) * TRAIN_FRACTION)\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "            # Break Training early if desired training fraction is reached\n",
    "            if batch_idx >= total_batches:\n",
    "                break\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print metrics\n",
    "        val_metrics = evaluate(model, val_loader, device, num_classes)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}, Validation Metrics: {val_metrics}\")\n",
    "        # Save the best model weights across all epochs\n",
    "        if val_metrics['mean_iou'] > best_val_iou:\n",
    "            best_val_iou = val_metrics['mean_iou']\n",
    "            best_metrics = val_metrics\n",
    "            torch.save(model.state_dict(), f\"best_model_{config[\"backbone\"]}_{config[\"output_layer_high\"]}_{config[\"output_layer_low\"]}.pth\")\n",
    "    \n",
    "    return best_metrics\n",
    "\n",
    "# Function to evaluate the performance metrics\n",
    "def evaluate(model, val_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    total_iou_per_class = np.zeros(num_classes)\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou_per_class = compute_iou(preds, targets, num_classes)\n",
    "            total_iou_per_class += np.nan_to_num(iou_per_class)\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "    \n",
    "    mean_iou_per_class = total_iou_per_class / len(val_loader)\n",
    "    mean_iou = np.nanmean(mean_iou_per_class)\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    return {\"mean_iou\": mean_iou, \"accuracy\": accuracy, \"f1_score\": f1, \"class_iou\": mean_iou_per_class.tolist()}\n",
    "\n",
    "# Function to manually compute the IoU\n",
    "def compute_iou(preds, targets, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        intersection = ((preds == cls) & (targets == cls)).sum().item()\n",
    "        union = ((preds == cls) | (targets == cls)).sum().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(np.nan)\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return iou_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through the experimental configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with config: {'backbone': 'resnet50', 'output_layer_high': 'layer4', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 757/757 [28:45<00:00,  2.28s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:49<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5290548138448395, Validation Metrics: {'mean_iou': 0.1739582516075541, 'accuracy': 0.8421132950530036, 'f1_score': 0.8360674306378592, 'class_iou': [0.0, 0.5590915472165695, 0.0, 0.0, 0.0006078201727154711, 0.0, 0.45820561555074224, 0.8367674474826029, 6.906314916727569e-05, 0.0, 0.009134198821373778, 0.0, 0.0, 0.0, 0.06284386460945039, 0.005633216279488564, 0.6339286105486445, 0.5649671451052191]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 757/757 [28:02<00:00,  2.22s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:54<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.3487351562537479, Validation Metrics: {'mean_iou': 0.18129346036478491, 'accuracy': 0.8510170494699647, 'f1_score': 0.8451913836007036, 'class_iou': [0.0, 0.6160892017729683, 0.0, 0.0, 0.0015019397999240117, 0.0, 0.48572259374269744, 0.8411310771566076, 0.010099187586213912, 0.0, 0.012485967410953565, 0.0, 0.0, 0.0, 0.08435723626398044, 0.009054872473402826, 0.6382877976532169, 0.5645524127061635]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 757/757 [28:05<00:00,  2.23s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:57<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2945696851103278, Validation Metrics: {'mean_iou': 0.17867544747882821, 'accuracy': 0.8456877760600706, 'f1_score': 0.8433651262355633, 'class_iou': [0.0, 0.5464308082803019, 0.0, 0.0, 0.0005968940334419477, 0.0, 0.4945735853916585, 0.8422806934158831, 0.014166953283923374, 0.0, 0.022447499300438236, 0.0, 0.0, 0.0, 0.0907664853516072, 0.00980435815223845, 0.6306202583317898, 0.5644705190776256]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 757/757 [28:14<00:00,  2.24s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:56<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.25515021391636156, Validation Metrics: {'mean_iou': 0.17998776279208215, 'accuracy': 0.8471141177120142, 'f1_score': 0.8451568351323852, 'class_iou': [0.0, 0.5595091717189109, 0.0, 0.0, 0.0006050738031458829, 0.0, 0.503425722328688, 0.8422096009848682, 0.024849086051835428, 0.0, 0.017851433281242227, 0.0, 0.0, 0.0, 0.08177979450380536, 0.014076403016665637, 0.6199560861539456, 0.5755173584143717]}\n",
      "Running experiment with config: {'backbone': 'resnet101', 'output_layer_high': 'layer4', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 757/757 [35:38<00:00,  2.82s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:58<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5729096755877511, Validation Metrics: {'mean_iou': 0.17699884306912372, 'accuracy': 0.8440756349381625, 'f1_score': 0.8388631058737326, 'class_iou': [0.0, 0.5672619592911528, 0.0, 0.0, 0.0005807843056108786, 0.0, 0.4696332818020108, 0.8384354862234713, 0.00041153067101642863, 0.0, 0.011465732524726678, 0.0, 0.0, 0.0, 0.08482649568307613, 0.012277856258393066, 0.6321193172388448, 0.5689667312459243]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 757/757 [35:27<00:00,  2.81s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:00<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.3460655860219008, Validation Metrics: {'mean_iou': 0.18170381765847549, 'accuracy': 0.8491220682420495, 'f1_score': 0.8445987838022346, 'class_iou': [0.0, 0.6068247634646092, 0.0, 0.0, 0.0009188527402424619, 0.0, 0.4863505340037952, 0.8401191559777014, 0.015766797872184133, 0.0, 0.020857254692516886, 0.0, 0.0, 0.0, 0.08997479241986667, 0.01447585978835979, 0.6381222686536989, 0.5572584382395842]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 757/757 [35:33<00:00,  2.82s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:03<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2918993609033456, Validation Metrics: {'mean_iou': 0.18456825272090846, 'accuracy': 0.8520105565371024, 'f1_score': 0.8489384747824524, 'class_iou': [0.0, 0.6021493082209688, 0.0, 0.0, 0.001132443550178612, 0.0, 0.504243628068186, 0.843423684893764, 0.022507889618699144, 0.0, 0.02473269393922742, 0.0, 0.0, 0.0, 0.1022257994051618, 0.015317738125729246, 0.6375437397417856, 0.5689516234126518]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 757/757 [35:30<00:00,  2.81s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:01<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.2569111863168417, Validation Metrics: {'mean_iou': 0.18058369893927695, 'accuracy': 0.8469532575088339, 'f1_score': 0.8442150427465841, 'class_iou': [0.0, 0.5687016380966664, 0.0, 0.0, 0.0006219111439853096, 0.0, 0.4991325355499748, 0.8439697673285216, 0.01590326377082026, 0.0, 0.03284351984021219, 0.0, 0.0, 0.0, 0.08428341961450235, 0.014071572461192085, 0.6218253833127558, 0.5691535697883542]}\n",
      "Running experiment with config: {'backbone': 'resnet50', 'output_layer_high': 'layer3', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 757/757 [28:04<00:00,  2.23s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:55<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5409465094381154, Validation Metrics: {'mean_iou': 0.17907080086009333, 'accuracy': 0.8492385711130742, 'f1_score': 0.8462843191020996, 'class_iou': [0.0, 0.5617384659860583, 0.0, 0.0, 0.0005692741703163306, 0.0, 0.5152392227919677, 0.8460883832506503, 3.069877030457266e-05, 0.0, 0.018120707889262885, 0.0, 0.0, 0.0, 0.07626838642971334, 0.0, 0.6355078419748316, 0.5697114342185753]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 757/757 [28:00<00:00,  2.22s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:55<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.3464593748750271, Validation Metrics: {'mean_iou': 0.18347130720872662, 'accuracy': 0.8548314211572439, 'f1_score': 0.8520396128179081, 'class_iou': [0.0, 0.5788132025709126, 0.0, 0.0, 0.000616721988181396, 0.0, 0.5375748656193703, 0.8498227460096861, 0.005229686176742266, 0.0, 0.018450069426673276, 0.0, 0.0, 0.0, 0.08591216877413278, 0.007306065212524009, 0.6423037984693954, 0.5764542055094606]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 757/757 [28:01<00:00,  2.22s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [00:54<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.31213464425529797, Validation Metrics: {'mean_iou': 0.18339193936999137, 'accuracy': 0.8537919887367491, 'f1_score': 0.8517695365495741, 'class_iou': [0.0, 0.5785006322676575, 0.0, 0.0, 0.0006087949297954551, 0.0, 0.538946910343594, 0.849084850922203, 0.01590927625010066, 0.0, 0.020499444171209528, 0.0, 0.0, 0.0033672891907187323, 0.08403860555042501, 0.0073395964556421485, 0.6421759550365492, 0.5605835535419497]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 757/757 [28:04<00:00,  2.23s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:02<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.28281824420670065, Validation Metrics: {'mean_iou': 0.1834707726995304, 'accuracy': 0.8522773078621908, 'f1_score': 0.8520318090900258, 'class_iou': [0.0, 0.5503010807435238, 0.0, 0.0, 0.0005783204857278931, 0.0, 0.5387008910677895, 0.8505573617176075, 0.022347206806299827, 0.0, 0.02883003219813753, 0.0, 0.0, 0.0038777032065622666, 0.0946008972391352, 0.00456713304741147, 0.6424863359082951, 0.5656269461710574]}\n",
      "Running experiment with config: {'backbone': 'resnet101', 'output_layer_high': 'layer3', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 757/757 [35:15<00:00,  2.80s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5249223810284739, Validation Metrics: {'mean_iou': 0.18320535710286007, 'accuracy': 0.8508658734540636, 'f1_score': 0.8488046724746044, 'class_iou': [0.0, 0.578501010358892, 0.0, 0.0, 0.0005779987190739648, 0.0, 0.5251002296549848, 0.8460416341136783, 0.00031523225042525457, 0.0, 0.019102823830561645, 0.0, 0.0, 0.0, 0.10079220292454392, 0.013698041475819253, 0.6433294712313514, 0.5702377832921507]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 757/757 [35:13<00:00,  2.79s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.33443515546019514, Validation Metrics: {'mean_iou': 0.18741579824464066, 'accuracy': 0.8567411605565372, 'f1_score': 0.8547944051525402, 'class_iou': [0.0, 0.5958474386099571, 0.0, 0.0, 0.000703845942033955, 0.0, 0.5464750424965014, 0.851115169536453, 0.007057597135578117, 0.0, 0.02410394408537695, 0.0, 0.0, 0.0, 0.1073477601321176, 0.014323770491803278, 0.6455982917937341, 0.580911508179976]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 757/757 [35:22<00:00,  2.80s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:02<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.29316964184513494, Validation Metrics: {'mean_iou': 0.18549826580727566, 'accuracy': 0.85282761704947, 'f1_score': 0.8534718662788048, 'class_iou': [0.0, 0.5531995520191597, 0.0, 0.0, 0.0006076436837615243, 0.0, 0.5494103362376799, 0.8495820714269072, 0.021017906891624173, 0.0, 0.025020060644421922, 0.0, 0.0, 0.0, 0.1090985798801154, 0.01569611265099408, 0.6425946926319981, 0.5727418284642999]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 757/757 [35:55<00:00,  2.85s/it]\n",
      "Evaluating: 100%|██████████| 36/36 [01:04<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.26186677051740087, Validation Metrics: {'mean_iou': 0.18420417387073423, 'accuracy': 0.8542459750441697, 'f1_score': 0.8525610554196401, 'class_iou': [0.0, 0.5335501697556532, 0.0, 0.0, 0.0006726891691802028, 0.0, 0.5442503513607826, 0.8530091498251285, 0.014694408860222012, 0.0, 0.02331869098493842, 0.0, 0.0, 0.0, 0.10732931267136286, 0.015268896540416462, 0.6357052216878669, 0.5878762388176653]}\n",
      "Experiments completed. Results saved to experiment_results.json\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Loop through each backbone configuration and retrain the model\n",
    "for config in experiments:\n",
    "    print(f\"Running experiment with config: {config}\")\n",
    "    val_metrics = train_and_evaluate(config, train_loader, val_loader, NUM_CLASSES, DEVICE, epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)\n",
    "    results.append({\"config\": config, \"metrics\": val_metrics})\n",
    "\n",
    "# Save results to a file\n",
    "with open(\"experiment_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Experiments completed. Results saved to experiment_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of class label indeces/RGB values (from Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "METAINFO = {\n",
    "    \"classes\": (\n",
    "        \"unlabelled\",\n",
    "        \"asphalt\",\n",
    "        \"dirt\",\n",
    "        \"mud\",\n",
    "        \"water\",\n",
    "        \"gravel\",\n",
    "        \"other-terrain\",\n",
    "        \"tree-trunk\",\n",
    "        \"tree-foliage\",\n",
    "        \"bush\",\n",
    "        \"fence\",\n",
    "        \"structure\",\n",
    "        \"pole\",\n",
    "        \"vehicle\",\n",
    "        \"rock\",\n",
    "        \"log\",\n",
    "        \"other-object\",\n",
    "        \"sky\",\n",
    "        \"grass\",\n",
    "    ),\n",
    "    \"palette\": [\n",
    "        (0, 0, 0),\n",
    "        (230, 25, 75),\n",
    "        (60, 180, 75),\n",
    "        (255, 225, 25),\n",
    "        (0, 130, 200),\n",
    "        (145, 30, 180),\n",
    "        (70, 240, 240),\n",
    "        (240, 50, 230),\n",
    "        (210, 245, 60),\n",
    "        (230, 25, 75),\n",
    "        (0, 128, 128),\n",
    "        (170, 110, 40),\n",
    "        (255, 250, 200),\n",
    "        (128, 0, 0),\n",
    "        (170, 255, 195),\n",
    "        (128, 128, 0),\n",
    "        (250, 190, 190),\n",
    "        (0, 0, 128),\n",
    "        (128, 128, 128),\n",
    "    ],\n",
    "        \"cidx\": [\n",
    "            0,\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "            5,\n",
    "            6,\n",
    "            7,\n",
    "            8,\n",
    "            9,\n",
    "            10,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "            14,\n",
    "            15,\n",
    "            16,\n",
    "            17,\n",
    "            18\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with config: {'backbone': 'resnet50', 'output_layer_high': 'layer4', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/czytw7350vv52f8_xwb_zl_w0000gn/T/ipykernel_66962/3815997514.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Testing: 100%|██████████| 267/267 [05:51<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 0.19008691577114056, 'accuracy': 0.8445787798874824, 'f1_score': 0.8360513886814105, 'class_iou': [0.0, 0.6304506815910224, 0.00023967924877025043, 0.029427619695273435, 4.138473317193288e-06, 0.0, 0.4817987515783103, 0.8420911469038814, 0.031030748976616323, 0.0, 0.03788944113119673, 0.0, 0.0, 0.01397874935240465, 0.13365205193324425, 0.0479065291767016, 0.6056355423286769, 0.5674594034911149]}\n",
      "Running experiment with config: {'backbone': 'resnet101', 'output_layer_high': 'layer4', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/czytw7350vv52f8_xwb_zl_w0000gn/T/ipykernel_66962/3815997514.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Testing: 100%|██████████| 267/267 [06:53<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 0.19310884108787235, 'accuracy': 0.8445811181434599, 'f1_score': 0.839494745385048, 'class_iou': [0.0, 0.6101475659231544, 0.0031461536980999312, 0.026887461420761018, 0.002333143788986656, 4.208033404607587e-05, 0.4995293403954374, 0.8428774556484061, 0.04732471705020927, 0.0008013488699387541, 0.0395954850502689, 0.0, 0.0, 0.013094932634762203, 0.15032935822903623, 0.06130080640401789, 0.6070629753297718, 0.5714863148048053]}\n",
      "Running experiment with config: {'backbone': 'resnet50', 'output_layer_high': 'layer3', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/czytw7350vv52f8_xwb_zl_w0000gn/T/ipykernel_66962/3815997514.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Testing: 100%|██████████| 267/267 [06:03<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 0.1928018330619099, 'accuracy': 0.8464796977555087, 'f1_score': 0.8411083588394737, 'class_iou': [0.0, 0.6054698931002198, 0.0, 0.026803288055493563, 0.001039225143950104, 0.0, 0.5249150655917117, 0.850248274048556, 0.020234899963034212, 0.0, 0.030847333905563226, 0.0, 0.0, 0.020061711949647766, 0.14419808665454964, 0.05241936681813284, 0.6147813909156281, 0.5794144589678912]}\n",
      "Running experiment with config: {'backbone': 'resnet101', 'output_layer_high': 'layer3', 'output_layer_low': 'layer1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/czytw7350vv52f8_xwb_zl_w0000gn/T/ipykernel_66962/3815997514.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "Testing: 100%|██████████| 267/267 [06:53<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_iou': 0.19945503354982563, 'accuracy': 0.8532773258028599, 'f1_score': 0.8458698779788422, 'class_iou': [0.0, 0.6465552159972368, 0.0, 0.032013878437491254, 0.00030508743389685996, 0.0, 0.5339058524221552, 0.8520785341515528, 0.015775702300860592, 0.0, 0.03573066912369483, 0.0, 0.0, 0.0170214526311022, 0.1773166136526842, 0.07790119778358737, 0.6188645346354121, 0.5827218653271867]}\n",
      "Testing Complete. Results saved to experiment_results_test_set.json\n"
     ]
    }
   ],
   "source": [
    "# Fraction of the testing file (testcsv) to use for testing models\n",
    "TEST_FRACTION = 1\n",
    "# Choose to save image predictions\n",
    "SAVE_IMAGES = False\n",
    "\n",
    "# Function to carry out testing of image segmentation\n",
    "def SegmentScenes(test_loader, model_path, num_classes, config, device, save_images=False):\n",
    "    # Instantiate DeepLabV3+ model and load saved weights\n",
    "    model = Deeplabv3Plus(num_classes=num_classes, \n",
    "                          backbone_name=config[\"backbone\"], \n",
    "                          output_layer_high=config[\"output_layer_high\"], \n",
    "                          output_layer_low=config[\"output_layer_low\"]).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    total_iou_per_class = np.zeros(num_classes)\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    # Main testing loop (with break condition if TEST_FRACTION reached)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            if batch_idx >= len(test_loader) * TEST_FRACTION:\n",
    "                break\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            iou_per_class = compute_iou(preds, targets, num_classes)\n",
    "            total_iou_per_class += np.nan_to_num(iou_per_class)\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            \n",
    "            # Save images only if desired\n",
    "            if save_images:\n",
    "                save_predictions(data, preds, targets)\n",
    "    \n",
    "    # Peformance metrics\n",
    "    mean_iou_per_class = total_iou_per_class / (len(test_loader) * TEST_FRACTION)\n",
    "    mean_iou = np.nanmean(mean_iou_per_class)\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    return {\"mean_iou\": mean_iou, \"accuracy\": accuracy, \"f1_score\": f1, \"class_iou\": mean_iou_per_class.tolist()}\n",
    "\n",
    "def save_predictions(data, preds, targets):\n",
    "    for i in range(data.size(0)):\n",
    "        image = data[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        preds_resized = preds[i].cpu().numpy()\n",
    "        filename = f\"test_image_{i}.png\"\n",
    "        \n",
    "        # Save prediction (RGB label format)\n",
    "        pred_rgb = np.zeros((*preds_resized.shape, 3), dtype=np.uint8)\n",
    "        for class_idx, color in enumerate(METAINFO['palette']):\n",
    "            pred_rgb[preds_resized == class_idx] = color\n",
    "        pred_image = Image.fromarray(pred_rgb)\n",
    "        pred_filename = f\"prediction_{filename}\"\n",
    "        pred_image.save(os.path.join(PREDICTIONS, pred_filename))\n",
    "\n",
    "        # Save prediction (index label format)\n",
    "        pred_label = np.zeros((*preds_resized.shape, 3), dtype=np.uint8)\n",
    "        for class_idx, color in enumerate(METAINFO['palette']):\n",
    "            pred_label[preds_resized == class_idx] = color\n",
    "        pred_label[..., 0] = preds_resized  # Red channel\n",
    "        pred_label[..., 1] = preds_resized  # Green channel\n",
    "        pred_label[..., 2] = preds_resized  # Blue channel\n",
    "        pred_label_image = Image.fromarray(pred_label)\n",
    "        pred_label_filename = f\"prediction_label_{filename}\"\n",
    "        pred_label_image.save(os.path.join(PREDICTIONS_LABELS, pred_label_filename))\n",
    "\n",
    "# Loop through each backbone configuration and test the saved model\n",
    "results = []\n",
    "for exp_id, config in enumerate(experiments):\n",
    "    model_path = f\"best_model_{config['backbone']}_{config['output_layer_high']}_{config['output_layer_low']}.pth\"\n",
    "    config = experiments[exp_id]\n",
    "    print(\"Running experiment with config:\", config)\n",
    "    val_metrics = SegmentScenes(test_loader, model_path, NUM_CLASSES, config, DEVICE, save_images=SAVE_IMAGES)\n",
    "    print(val_metrics)\n",
    "    results.append({\"config\": config, \"metrics\": val_metrics})\n",
    "\n",
    "# Save results to a file\n",
    "with open(\"experiment_results_test_set.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Testing Complete. Results saved to experiment_results_test_set.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OPTIONAL) Check the prediction set class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 2133/2133 [03:26<00:00, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pixel values across all predictions: {2, 4, 5, 7, 8, 9, 11, 14, 15, 16, 17, 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "def get_all_unique_pixel_values(directory_path):\n",
    "    unique_values_set = set()\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in tqdm(os.listdir(directory_path), desc=f\"Processing Images\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        # Check if the file is an image file\n",
    "        if filename.endswith(\".png\"):\n",
    "            # Open the image\n",
    "            image = Image.open(file_path)\n",
    "            # Convert the image to a NumPy array\n",
    "            image_array = np.array(image)\n",
    "            # Get unique pixel values and add them to the set\n",
    "            unique_values = np.unique(image_array)\n",
    "            unique_values_set.update(unique_values)\n",
    "    return unique_values_set\n",
    "\n",
    "unique_pixel_values = get_all_unique_pixel_values(PREDICTIONS_LABELS)\n",
    "# Print the unique pixel values across all images\n",
    "print(f\"Unique pixel values across all predictions: {unique_pixel_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 2133/2133 [02:55<00:00, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pixel values across same GT labels (indexLabel for test.csv): {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_unique_pixel_values2(directory_path):\n",
    "    unique_values_set = set()\n",
    "    # Iterate over each file in the directory\n",
    "    for idx in tqdm(range(len(directory_path)), desc=f\"Processing Images\"):\n",
    "        # Construct the full file path\n",
    "        label_path = test_paths.iloc[idx, 2]\n",
    "        image = Image.open(f\"{WILDSCENES_PATH}/{label_path}\").convert(\"RGB\")\n",
    "        image_array = np.array(image)\n",
    "        # Get unique pixel values and add them to the set\n",
    "        unique_values = np.unique(image_array)\n",
    "        unique_values_set.update(unique_values)\n",
    "    return unique_values_set\n",
    "\n",
    "test_paths = pd.read_csv(TEST_CSV)\n",
    "unique_pixel_values = get_all_unique_pixel_values2(test_paths)\n",
    "# Print the unique pixel values across all images\n",
    "print(f\"Unique pixel values across same GT labels (indexLabel for test.csv): {unique_pixel_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
